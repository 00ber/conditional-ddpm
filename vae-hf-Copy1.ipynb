{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b58af06-928f-41eb-af13-8d6933c9cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 17:42:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   40C    P8             23W /  230W |    3136MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   37C    P8             19W /  230W |       3MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000               Off |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   38C    P8             19W /  230W |       3MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000               Off |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   36C    P8             22W /  230W |       3MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   4052052      C   ...atch/miniconda3/envs/llm/bin/python       1276MiB |\n",
      "|    0   N/A  N/A   4052620      C   ...atch/miniconda3/envs/llm/bin/python       1148MiB |\n",
      "|    0   N/A  N/A   4053425      C   ...atch/miniconda3/envs/llm/bin/python        702MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27418fc-e643-4fb4-81cd-3cf35bb50a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import make_grid\n",
    "from diffusers import AutoencoderKL\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "#############################\n",
    "im_channels = 3\n",
    "\n",
    "# Create the model and dataset #\n",
    "# params = {\n",
    "#     \"block_out_channels\": [\n",
    "#     128,\n",
    "#     256,\n",
    "#     512,\n",
    "#     512\n",
    "#   ],\n",
    "#   \"down_block_types\": [\n",
    "#     \"DownEncoderBlock2D\",\n",
    "#     \"DownEncoderBlock2D\",\n",
    "#     \"DownEncoderBlock2D\",\n",
    "#     \"DownEncoderBlock2D\"\n",
    "#   ],\n",
    "#   \"in_channels\": 3,\n",
    "#   \"latent_channels\": 4,\n",
    "#   \"layers_per_block\": 2,\n",
    "#   \"out_channels\": 3,\n",
    "#   \"sample_size\": 32,\n",
    "#   \"scaling_factor\": 0.18215,\n",
    "#   \"up_block_types\": [\n",
    "#     \"UpDecoderBlock2D\",\n",
    "#     \"UpDecoderBlock2D\",\n",
    "#     \"UpDecoderBlock2D\",\n",
    "#     \"UpDecoderBlock2D\"\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# model = AutoencoderKL(**params).to(device)\n",
    "model = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
    "\n",
    "# model = AutoencoderKL(in_channels=im_channels, out_channels=im_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0695a953-711c-473b-babd-d54a4d23c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f96ef0bef5456fa11145bacc3d6b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2193\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from datasets import load_dataset\n",
    "\n",
    "resolution = 256\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# augmentations = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.RandomCrop(resolution),\n",
    "#         transforms.Lambda(lambda x: x),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5], [0.5]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "train_batch_size = 4\n",
    "dataset_path = \"ddpm-data/summer2winter_yosemite/train\"\n",
    "train_ds = load_dataset(\"imagefolder\", data_dir=dataset_path, split=\"train\")\n",
    "print(f\"Dataset size: {len(train_ds)}\")\n",
    "train_ds.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c584b540-a950-40f6-9b77-d3f31a6d00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(model, x):\n",
    "#     # z_mu, z_var = Q(X)\n",
    "#     # z = sample_z(z_mu, z_var)\n",
    "#     # X_sample = P(z)\n",
    "\n",
    "#     # # Loss\n",
    "#     # recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "#     # posterior = model.encode(x).latent_dist\n",
    "#     # print(posterior)\n",
    "#     # if sample_posterior:\n",
    "#     #     z = posterior.sample(generator=generator)\n",
    "#     # else:\n",
    "#     #     z = posterior.mode()\n",
    "#     # dec = self.decode(z).sample\n",
    "\n",
    "#     # if not return_dict:\n",
    "#     #     return (dec,)\n",
    "\n",
    "#     # return DecoderOutput(sample=dec)\n",
    "\n",
    "\n",
    "#     posterior = model.encode(x).latent_dist\n",
    "#     z = posterior.mode()\n",
    "#     # z = posterior.sample()\n",
    "    \n",
    "#     x_sample = model.decode(z).sample\n",
    "#     # Loss\n",
    "#     recon_loss = F.mse_loss(x_sample, x)\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     kl_loss = posterior.kl()\n",
    "#     loss = recon_loss + kl_loss\n",
    "#     return x_sample,  loss.mean(), recon_loss.mean(), kl_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae4798d-8ba2-4f91-bdac-a3f402d7e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forwardV2(model, x):\n",
    "#     # z_mu, z_var = Q(X)\n",
    "#     # z = sample_z(z_mu, z_var)\n",
    "#     # X_sample = P(z)\n",
    "\n",
    "#     # # Loss\n",
    "#     # recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "#     # posterior = model.encode(x).latent_dist\n",
    "#     # print(posterior)\n",
    "#     # if sample_posterior:\n",
    "#     #     z = posterior.sample(generator=generator)\n",
    "#     # else:\n",
    "#     #     z = posterior.mode()\n",
    "#     # dec = self.decode(z).sample\n",
    "\n",
    "#     # if not return_dict:\n",
    "#     #     return (dec,)\n",
    "\n",
    "#     # return DecoderOutput(sample=dec)\n",
    "\n",
    "\n",
    "#     posterior = model.encode(x).latent_dist\n",
    "#     z = posterior.mode()\n",
    "#     # z = posterior.sample()\n",
    "    \n",
    "#     x_sample = model.decode(z).sample\n",
    "#     # print(x_sample)\n",
    "#     # Loss\n",
    "#     # recon_loss = F.mse_loss(x_sample, x)\n",
    "    \n",
    "#     recon_loss = 0.5 * F.mse_loss(\n",
    "#         x_sample.reshape(x.shape[0], -1),\n",
    "#         x.reshape(x.shape[0], -1),\n",
    "#         reduction=\"none\",\n",
    "#     ).sum(dim=-1)\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # kl_loss = posterior.kl()\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "\n",
    "\n",
    "#     # return 0.5 * torch.sum(\n",
    "#     #                 torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,\n",
    "#     #                 dim=[1, 2, 3],\n",
    "#     #             )\n",
    "\n",
    "\n",
    "#     # print(posterior.logvar.shape, posterior.mean.shape)\n",
    "#     KLD = 0.5 * torch.sum(\n",
    "#         torch.pow(posterior.mean, 2) + posterior.var - 1.0 - posterior.logvar,\n",
    "#         dim=[1, 2, 3],\n",
    "#     )\n",
    "\n",
    "#     loss = (recon_loss + KLD).mean(dim=0)\n",
    "#     return x_sample, loss , recon_loss.mean(dim=0), KLD.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538b0487-aab5-4294-9ba4-5b36f0dc9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f40e15-f41f-478e-9dd1-4dbf1dfb15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = forwardV2(model, batch[\"input\"].to(device))\n",
    "# out[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0616ebe3-f34e-4c5d-ba53-885033a0ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc093cc-fba3-48fe-acdf-afb559626a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _x = out[0][0].detach().cpu()\n",
    "# plt.imshow(_x.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848cc58e-4902-477a-ae87-1087ec255fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_img(img_tensor):\n",
    "#     img_tensor = (img_tensor - img_tensor.min()) / (img_tensor.max() - img_tensor.min())\n",
    "#     img_tensor = torch.clamp(img_tensor, 0, 1) * 255\n",
    "    \n",
    "#     # Convert to numpy and display using matplotlib\n",
    "#     image = img_tensor.numpy().transpose(1, 2, 0)  # Assuming the format is CxHxW, convert to HxWxC\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
    "#     plt.show()\n",
    "#     return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea47463-c5fe-4a84-a832-6fde1c80e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_img(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8713d-4a5f-4385-ba15-94cc75b09053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader):\n",
    "    from tqdm.auto import tqdm, trange\n",
    "    import os\n",
    "    from diffusers.optimization import get_scheduler\n",
    "    import lpips\n",
    "\n",
    "    task_name = \"summer-2-winter-vae-hf\"\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    model = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        betas=(0.95, 0.999),\n",
    "        weight_decay=1e-6,\n",
    "        eps=1e-08,\n",
    "    )\n",
    "\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100 * gradient_accumulation_steps,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    step_count = 0\n",
    "    image_save_steps = 64\n",
    "    acc_steps = 4\n",
    "    img_save_count = 0\n",
    "    vae_autoencoder_ckpt_name = 'vqvae_autoencoder_ckpt.pth'\n",
    "\n",
    "    kl_scale = 1e-6\n",
    "    lpips_scale = 1e-1\n",
    "    lpips_loss_fn = lpips.LPIPS(net=\"alex\").to(device)\n",
    "\n",
    "    with trange(num_epochs, desc=\"Epoch\") as ep:\n",
    "        for epoch_idx in ep:\n",
    "            losses = []\n",
    "            mse_losses = []\n",
    "            kl_losses = []\n",
    "            lpips_losses = []\n",
    "            optimizer.zero_grad()\n",
    "            with tqdm(train_dataloader, desc=\"Train\") as tr_loop:\n",
    "                for batch in tr_loop:\n",
    "                    step_count += 1\n",
    "                    x = batch[\"input\"].to(device)\n",
    "\n",
    "                    posterior = model.encode(x).latent_dist\n",
    "                    z = posterior.mode()\n",
    "                    pred = model.decode(z).sample\n",
    "                    kl_loss = posterior.kl().mean()\n",
    "                    mse_loss = F.mse_loss(pred, x, reduction=\"mean\")\n",
    "                    lpips_loss = lpips_loss_fn(pred, x).mean()\n",
    "\n",
    "                    loss = (\n",
    "                        mse_loss + lpips_scale * lpips_loss + kl_scale * kl_loss\n",
    "                    )\n",
    "                    \n",
    "                    # Image Saving Logic\n",
    "                    if step_count % image_save_steps == 0 or step_count == 1:\n",
    "                        sample_size = min(8, x.shape[0])\n",
    "                        save_output = torch.clamp(pred[:sample_size], -1., 1.).detach().cpu()\n",
    "                        save_output = ((save_output + 1) / 2)\n",
    "                        save_input = ((x[:sample_size] + 1) / 2).detach().cpu()\n",
    "                        \n",
    "                        grid = make_grid(torch.cat([save_input, save_output], dim=0), nrow=sample_size)\n",
    "                        img = transforms.ToPILImage()(grid)\n",
    "                        if not os.path.exists(os.path.join(task_name,'vqvae_autoencoder_samples')):\n",
    "                            os.mkdir(os.path.join(task_name, 'vqvae_autoencoder_samples'))\n",
    "                        img.save(os.path.join(task_name,'vqvae_autoencoder_samples',\n",
    "                                            'current_autoencoder_sample_{}.png'.format(img_save_count)))\n",
    "                        img_save_count += 1\n",
    "                        img.close()\n",
    "                    tr_loop.set_postfix(loss=loss.item(), mse_loss=mse_loss.item(), kl_loss=kl_loss.item(), lpips_loss=lpips_loss.item())\n",
    "                    losses.append(loss.item())\n",
    "                    mse_losses.append(mse_loss.item())\n",
    "                    kl_losses.append(kl_loss.item())\n",
    "                    lpips_losses.append(lpips_loss.item())\n",
    "                    loss.backward()\n",
    "                \n",
    "                    \n",
    "                    if step_count % acc_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_epoch_loss = sum(losses) / len(losses)\n",
    "            avg_epoch_mse_loss = sum(mse_losses) / len(mse_losses)\n",
    "            avg_epoch_kl_loss = sum(kl_losses) / len(kl_losses)\n",
    "            avg_epoch_lpips_loss = sum(lpips_losses) / len(lpips_losses)\n",
    "            ep.set_postfix(\n",
    "                avg_epoch_loss=avg_epoch_loss, \n",
    "                avg_epoch_mse_loss=avg_epoch_mse_loss, \n",
    "                avg_epoch_kl_loss=avg_epoch_kl_loss,\n",
    "                avg_epoch_lpips_loss=avg_epoch_lpips_loss\n",
    "            )\n",
    "            \n",
    "            torch.save(model.state_dict(), os.path.join(task_name, vae_autoencoder_ckpt_name))\n",
    "    print('Done Training...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5010fd8d-6478-431a-94fb-da5bb669bbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f248f51a1a6417b80d581cd7b448dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0f44d353744d75af32e250b657025a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tr_loop:\n\u001b[1;32m     45\u001b[0m     step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(x)\u001b[38;5;241m.\u001b[39mlatent_dist\n\u001b[1;32m     49\u001b[0m     z \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mmode()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db306b-16e2-4b8a-b585-71e6813fe7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
