{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae59469-5329-4195-a43a-2643831037a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm.auto import trange, tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd7f2a7-3b69-415c-bb6b-f12383cca66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, use_residual: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.match_channels = input_channels == output_channels\n",
    "        self.use_residual = use_residual\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.second_conv = nn.Sequential(\n",
    "            nn.Conv2d(output_channels, output_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_residual:\n",
    "            conv1_output = self.first_conv(input_tensor)\n",
    "            conv2_output = self.second_conv(conv1_output)\n",
    "            if self.match_channels:\n",
    "                output = input_tensor + conv2_output\n",
    "            else:\n",
    "                output = conv1_output + conv2_output\n",
    "            return output / 1.414\n",
    "        else:\n",
    "            conv1_output = self.first_conv(input_tensor)\n",
    "            return self.second_conv(conv1_output)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        modules = [ConvBlock(input_channels, output_channels), nn.MaxPool2d(2)]\n",
    "        self.sequence = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.sequence(input_tensor)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        modules = [\n",
    "            nn.ConvTranspose2d(input_channels, output_channels, 2, 2),\n",
    "            ConvBlock(output_channels, output_channels),\n",
    "            ConvBlock(output_channels, output_channels),\n",
    "        ]\n",
    "        self.sequence = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, input_tensor, concatenated_tensor):\n",
    "        combined_tensor = torch.cat((input_tensor, concatenated_tensor), 1)\n",
    "        return self.sequence(combined_tensor)\n",
    "\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, input_dimension, embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        modules = [\n",
    "            nn.Linear(input_dimension, embedding_dimension),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dimension, embedding_dimension),\n",
    "        ]\n",
    "        self.sequence = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        reshaped_tensor = input_tensor.view(-1, self.input_dimension)\n",
    "        return self.sequence(reshaped_tensor)\n",
    "\n",
    "\n",
    "class EnhancedUnet(nn.Module):\n",
    "    def __init__(self, input_channels, feature_dim = 256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.initial_conv = ConvBlock(input_channels, feature_dim, use_residual=True)\n",
    "\n",
    "        self.down_block1 = DownBlock(feature_dim, feature_dim)\n",
    "        self.down_block2 = DownBlock(feature_dim, 2 * feature_dim)\n",
    "\n",
    "        self.flatten = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.temporal_embedding1 = FCBlock(1, 2*feature_dim)\n",
    "        self.temporal_embedding2 = FCBlock(1, feature_dim)\n",
    "        self.context_embedding1 = FCBlock(num_classes, 2*feature_dim)\n",
    "        self.context_embedding2 = FCBlock(num_classes, feature_dim)\n",
    "\n",
    "        self.up_sample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * feature_dim, 2 * feature_dim, kernel_size=16, stride=8, padding=4),\n",
    "            nn.GroupNorm(8, 2 * feature_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up_block1 = UpBlock(4 * feature_dim, feature_dim)\n",
    "        self.up_block2 = UpBlock(2 * feature_dim, feature_dim)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(2 * feature_dim, feature_dim, 3, 1, 1),\n",
    "            nn.GroupNorm(8, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_dim, self.input_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, context, time_step, mask):\n",
    "        processed_tensor = self.initial_conv(input_tensor)\n",
    "        down1_output = self.down_block1(processed_tensor)\n",
    "        down2_output = self.down_block2(down1_output)\n",
    "        hidden_vector = self.flatten(down2_output)\n",
    "\n",
    "        context_onehot = nn.functional.one_hot(context, num_classes=self.num_classes).type(torch.float)\n",
    "        masked_context = mask[:, None].repeat(1, self.num_classes)\n",
    "        masked_context = (-1 * (1 - masked_context))\n",
    "        masked_context = context_onehot * masked_context\n",
    "\n",
    "        context_emb1 = self.context_embedding1(masked_context).view(-1, self.feature_dim * 2, 1, 1)\n",
    "        time_emb1 = self.temporal_embedding1(time_step).view(-1, self.feature_dim * 2, 1, 1)\n",
    "        context_emb2 = self.context_embedding2(masked_context).view(-1, self.feature_dim, 1, 1)\n",
    "        time_emb2 = self.temporal_embedding2(time_step).view(-1, self.feature_dim, 1, 1)\n",
    "\n",
    "        up1_output = self.up_sample(hidden_vector)\n",
    "        up2_output = self.up_block1(context_emb1 * up1_output + time_emb1, down2_output)\n",
    "        final_output = self.up_block2(context_emb2 * up2_output + time_emb2, down1_output)\n",
    "        return self.final_conv(torch.cat((final_output, processed_tensor), 1))\n",
    "\n",
    "\n",
    "def schedule_ddpm(beta_start, beta_end, total_steps):\n",
    "    beta_range = (beta_end - beta_start) * torch.arange(0, total_steps + 1, dtype=torch.float32) / total_steps + beta_start\n",
    "    sqrt_beta = torch.sqrt(beta_range)\n",
    "    alpha = 1 - beta_range\n",
    "    log_alpha = torch.log(alpha)\n",
    "    cumulative_alpha = torch.cumsum(log_alpha, dim=0).exp()\n",
    "\n",
    "    sqrt_cum_alpha = torch.sqrt(cumulative_alpha)\n",
    "    one_over_sqrt_alpha = 1 / torch.sqrt(alpha)\n",
    "\n",
    "    sqrt_one_minus_cum_alpha = torch.sqrt(1 - cumulative_alpha)\n",
    "    one_minus_alpha_over_sqrt_one_minus_cum_alpha = (1 - alpha) / sqrt_one_minus_cum_alpha\n",
    "\n",
    "    return {\n",
    "        \"alpha\": alpha,\n",
    "        \"one_over_sqrt_alpha\": one_over_sqrt_alpha,\n",
    "        \"sqrt_beta\": sqrt_beta,\n",
    "        \"cumulative_alpha\": cumulative_alpha,\n",
    "        \"sqrt_cum_alpha\": sqrt_cum_alpha,\n",
    "        \"sqrt_one_minus_cum_alpha\": sqrt_one_minus_cum_alpha,\n",
    "        \"one_minus_alpha_over_sqrt_one_minus_cum_alpha\": one_minus_alpha_over_sqrt_one_minus_cum_alpha,\n",
    "    }\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, model, betas, num_steps, device, dropout_probability=0.1):\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "        for key, value in schedule_ddpm(betas[0], betas[1], num_steps).items():\n",
    "            self.register_buffer(key, value)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_tensor, context):\n",
    "        time_indices = torch.randint(1, self.num_steps+1, (input_tensor.shape[0],)).to(self.device)\n",
    "        random_noise = torch.randn_like(input_tensor)\n",
    "\n",
    "        processed_tensor = (\n",
    "            self.sqrt_cum_alpha[time_indices, None, None, None] * input_tensor\n",
    "            + self.sqrt_one_minus_cum_alpha[time_indices, None, None, None] * random_noise\n",
    "        )\n",
    "        context_dropout = torch.bernoulli(torch.zeros_like(context)+self.dropout_probability).to(self.device)\n",
    "        return self.mse_loss(random_noise, self.model(processed_tensor, context, time_indices / self.num_steps, context_dropout))\n",
    "\n",
    "    def sample(self, sample_count, dimensions, device, guidance_weight = 0.0):\n",
    "        initial_noise = torch.randn(sample_count, *dimensions).to(device)\n",
    "        context_indices = torch.arange(0, self.model.num_classes).to(device)\n",
    "        context_indices = context_indices.repeat(int(sample_count / context_indices.shape[0]))\n",
    "\n",
    "        context_mask = torch.zeros_like(context_indices).to(device)\n",
    "\n",
    "        context_indices = context_indices.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[sample_count:] = 1.0\n",
    "\n",
    "        stored_samples = []\n",
    "        for time_step in range(self.num_steps, 0, -1):\n",
    "            print(f'Sampling timestep {time_step}',end='\\r')\n",
    "            time_tensor = torch.tensor([time_step / self.num_steps]).to(device)\n",
    "            time_tensor = time_tensor.repeat(sample_count, 1, 1, 1)\n",
    "\n",
    "            initial_noise = initial_noise.repeat(2, 1, 1, 1)\n",
    "            time_tensor = time_tensor.repeat(2, 1, 1, 1)\n",
    "\n",
    "            additional_noise = torch.randn(sample_count, *dimensions).to(device) if time_step > 1 else 0\n",
    "\n",
    "            predicted_noise = self.model(initial_noise, context_indices, time_tensor, context_mask)\n",
    "            predicted_noise1 = predicted_noise[:sample_count]\n",
    "            predicted_noise2 = predicted_noise[sample_count:]\n",
    "            predicted_noise = (1 + guidance_weight) * predicted_noise1 - guidance_weight * predicted_noise2\n",
    "            initial_noise = initial_noise[:sample_count]\n",
    "            initial_noise = (\n",
    "                self.one_over_sqrt_alpha[time_step] * (initial_noise - predicted_noise * self.one_minus_alpha_over_sqrt_one_minus_cum_alpha[time_step])\n",
    "                + self.sqrt_beta[time_step] * additional_noise\n",
    "            )\n",
    "            if time_step % 20 == 0 or time_step == self.num_steps or time_step < 8:\n",
    "                stored_samples.append(initial_noise.detach().cpu().numpy())\n",
    "\n",
    "        stored_samples = np.array(stored_samples)\n",
    "        return initial_noise, stored_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175ca86-52fa-4d13-8e04-b77dae9c7390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2eac6516764a9b8caa5c47e8be35cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf71b760ea540d9bb5b6bd0d595d133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image at ./outputs-v1/image_epoch0_w0.0.png\n",
      "Saved image at ./outputs-v1/image_epoch0_w0.5.png\n",
      "Saved image at ./outputs-v1/image_epoch0_w2.0.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7157fe98c64d7994f56a94855bd6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a30ee4a5da4910a8446ba6a8a2354d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40c23fdf50d4a08abe6cfb8c4acf170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746f74a46cfc4b619603807cc76d44bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a2b428e934479991ea15556aa2f7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling timestep 388\r"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Configuration and Setup\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "steps = 500\n",
    "device_setting = \"cuda:2\"\n",
    "classes = 5\n",
    "features = 256\n",
    "rate = 1e-4\n",
    "save_check = True\n",
    "output_path = './outputs-v1/'\n",
    "weights = [0.0, 0.5, 2.0]\n",
    "\n",
    "# Model Initialization\n",
    "model = DiffusionModel(\n",
    "    model=EnhancedUnet(input_channels=3, feature_dim=features, num_classes=classes),\n",
    "    betas=[1e-4, 0.02], num_steps=steps, device=device_setting, dropout_probability=0.1)\n",
    "model.to(device_setting)\n",
    "\n",
    "# Data Loading\n",
    "path = \"transient-attr-images/train\"\n",
    "adjustment = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "data = ImageFolder(root=path, transform=adjustment)\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=rate)\n",
    "\n",
    "# Training\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    opt.param_groups[0]['lr'] = rate * (1 - epoch / epochs)\n",
    "    progress = tqdm(loader)\n",
    "    current_loss = None\n",
    "\n",
    "    for imgs, labels in progress:\n",
    "        opt.zero_grad()\n",
    "        imgs = imgs.to(device_setting)\n",
    "        labels = labels.to(device_setting)\n",
    "        loss = model(imgs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if current_loss is None:\n",
    "            current_loss = loss.item()\n",
    "        else:\n",
    "            current_loss = 0.95 * current_loss + 0.05 * loss.item()\n",
    "        progress.set_description(f\"Loss: {current_loss:.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample_count = 4 * classes\n",
    "            for weight_index, weight in enumerate(weights):\n",
    "                generated, _ = model.sample(sample_count, (3, 128, 128), device_setting, weight)\n",
    "                real_imgs = torch.Tensor(generated.shape).to(device_setting)\n",
    "                \n",
    "                for class_index in range(classes):\n",
    "                    for sample_index in range(sample_count // classes):\n",
    "                        try:\n",
    "                            index = torch.squeeze((labels == class_index).nonzero(as_tuple=False)[sample_index])\n",
    "                        except:\n",
    "                            index = 0\n",
    "                        real_imgs[class_index + (sample_index * classes)] = imgs[index]\n",
    "\n",
    "                combined = torch.cat([generated, real_imgs])\n",
    "                grid = make_grid(combined * -1 + 1, nrow=10)\n",
    "                save_image(grid, f\"{output_path}image_epoch{epoch}_w{weight}.png\")\n",
    "                print(f'Saved image at {output_path}image_epoch{epoch}_w{weight}.png')\n",
    "\n",
    "    if save_check and epoch == epochs - 1:\n",
    "        torch.save(model.state_dict(), f\"{output_path}model_epoch{epoch}.pth\")\n",
    "        print(f'Model saved at {output_path}model_epoch{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e6f29-e0eb-46dd-9404-b3d46f10f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16289f-d672-497c-b9df-c1e72a79f159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9d4e2-5a39-4be2-851a-f2a3ccd6a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c09b3-877e-4495-a652-9d262865b47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
