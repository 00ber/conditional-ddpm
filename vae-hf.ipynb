{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b58af06-928f-41eb-af13-8d6933c9cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 19:04:32 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   44C    P8             18W /  230W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   43C    P8             14W /  230W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000               Off |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   43C    P8             16W /  230W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000               Off |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   41C    P8             19W /  230W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27418fc-e643-4fb4-81cd-3cf35bb50a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0695a953-711c-473b-babd-d54a4d23c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353754809c5d4d41b1505a225b98d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825db85122d444dacd7376cd79be75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 2193\n",
      "Eval Dataset size: 547\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from datasets import load_dataset\n",
    "\n",
    "resolution = 256\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# augmentations = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.RandomCrop(resolution),\n",
    "#         transforms.Lambda(lambda x: x),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5], [0.5]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "train_batch_size = 4\n",
    "train_dataset_path = \"ddpm-data/summer2winter_yosemite/train\"\n",
    "eval_dataset_path = \"ddpm-data/summer2winter_yosemite/test\"\n",
    "train_ds = load_dataset(\"imagefolder\", data_dir=train_dataset_path, split=\"train\")\n",
    "eval_ds = load_dataset(\"imagefolder\", data_dir=eval_dataset_path, split=\"train\")\n",
    "print(f\"Train Dataset size: {len(train_ds)}\")\n",
    "print(f\"Eval Dataset size: {len(eval_ds)}\")\n",
    "train_ds.set_transform(transform_images)\n",
    "eval_ds.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_ds, batch_size=train_batch_size, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb107b6b-2444-4ecb-b478-b28a11271bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c584b540-a950-40f6-9b77-d3f31a6d00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(model, x):\n",
    "#     # z_mu, z_var = Q(X)\n",
    "#     # z = sample_z(z_mu, z_var)\n",
    "#     # X_sample = P(z)\n",
    "\n",
    "#     # # Loss\n",
    "#     # recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "#     # posterior = model.encode(x).latent_dist\n",
    "#     # print(posterior)\n",
    "#     # if sample_posterior:\n",
    "#     #     z = posterior.sample(generator=generator)\n",
    "#     # else:\n",
    "#     #     z = posterior.mode()\n",
    "#     # dec = self.decode(z).sample\n",
    "\n",
    "#     # if not return_dict:\n",
    "#     #     return (dec,)\n",
    "\n",
    "#     # return DecoderOutput(sample=dec)\n",
    "\n",
    "\n",
    "#     posterior = model.encode(x).latent_dist\n",
    "#     z = posterior.mode()\n",
    "#     # z = posterior.sample()\n",
    "    \n",
    "#     x_sample = model.decode(z).sample\n",
    "#     # Loss\n",
    "#     recon_loss = F.mse_loss(x_sample, x)\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     kl_loss = posterior.kl()\n",
    "#     loss = recon_loss + kl_loss\n",
    "#     return x_sample,  loss.mean(), recon_loss.mean(), kl_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae4798d-8ba2-4f91-bdac-a3f402d7e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forwardV2(model, x):\n",
    "#     # z_mu, z_var = Q(X)\n",
    "#     # z = sample_z(z_mu, z_var)\n",
    "#     # X_sample = P(z)\n",
    "\n",
    "#     # # Loss\n",
    "#     # recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "#     # posterior = model.encode(x).latent_dist\n",
    "#     # print(posterior)\n",
    "#     # if sample_posterior:\n",
    "#     #     z = posterior.sample(generator=generator)\n",
    "#     # else:\n",
    "#     #     z = posterior.mode()\n",
    "#     # dec = self.decode(z).sample\n",
    "\n",
    "#     # if not return_dict:\n",
    "#     #     return (dec,)\n",
    "\n",
    "#     # return DecoderOutput(sample=dec)\n",
    "\n",
    "\n",
    "#     posterior = model.encode(x).latent_dist\n",
    "#     z = posterior.mode()\n",
    "#     # z = posterior.sample()\n",
    "    \n",
    "#     x_sample = model.decode(z).sample\n",
    "#     # print(x_sample)\n",
    "#     # Loss\n",
    "#     # recon_loss = F.mse_loss(x_sample, x)\n",
    "    \n",
    "#     recon_loss = 0.5 * F.mse_loss(\n",
    "#         x_sample.reshape(x.shape[0], -1),\n",
    "#         x.reshape(x.shape[0], -1),\n",
    "#         reduction=\"none\",\n",
    "#     ).sum(dim=-1)\n",
    "#     # kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#     # kl_loss = posterior.kl()\n",
    "#     # loss = recon_loss + kl_loss\n",
    "\n",
    "\n",
    "\n",
    "#     # return 0.5 * torch.sum(\n",
    "#     #                 torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,\n",
    "#     #                 dim=[1, 2, 3],\n",
    "#     #             )\n",
    "\n",
    "\n",
    "#     # print(posterior.logvar.shape, posterior.mean.shape)\n",
    "#     KLD = 0.5 * torch.sum(\n",
    "#         torch.pow(posterior.mean, 2) + posterior.var - 1.0 - posterior.logvar,\n",
    "#         dim=[1, 2, 3],\n",
    "#     )\n",
    "\n",
    "#     loss = (recon_loss + KLD).mean(dim=0)\n",
    "#     return x_sample, loss , recon_loss.mean(dim=0), KLD.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538b0487-aab5-4294-9ba4-5b36f0dc9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f40e15-f41f-478e-9dd1-4dbf1dfb15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = forwardV2(model, batch[\"input\"].to(device))\n",
    "# out[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0616ebe3-f34e-4c5d-ba53-885033a0ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc093cc-fba3-48fe-acdf-afb559626a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _x = out[0][0].detach().cpu()\n",
    "# plt.imshow(_x.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848cc58e-4902-477a-ae87-1087ec255fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_img(img_tensor):\n",
    "#     img_tensor = (img_tensor - img_tensor.min()) / (img_tensor.max() - img_tensor.min())\n",
    "#     img_tensor = torch.clamp(img_tensor, 0, 1) * 255\n",
    "    \n",
    "#     # Convert to numpy and display using matplotlib\n",
    "#     image = img_tensor.numpy().transpose(1, 2, 0)  # Assuming the format is CxHxW, convert to HxWxC\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
    "#     plt.show()\n",
    "#     return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cea47463-c5fe-4a84-a832-6fde1c80e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_img(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d8713d-4a5f-4385-ba15-94cc75b09053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, eval_dataloader):\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm, trange\n",
    "    import os\n",
    "    from diffusers.optimization import get_scheduler\n",
    "    import lpips\n",
    "    import random\n",
    "    from torch.optim import Adam\n",
    "    from torchvision.utils import make_grid\n",
    "    from diffusers import AutoencoderKL\n",
    "    from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "    from accelerate.utils import ProjectConfiguration\n",
    "    from diffusers.utils import pt_to_pil, make_image_grid\n",
    "\n",
    "        \n",
    "    task_name = \"summer-2-winter-vae-hf\"\n",
    "    num_epochs = 20\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    model = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        betas=(0.95, 0.999),\n",
    "        weight_decay=1e-6,\n",
    "        eps=1e-08,\n",
    "    )\n",
    "\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100 * gradient_accumulation_steps,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "    lpips_loss_fn = lpips.LPIPS(net=\"alex\")\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision='fp16' ,\n",
    "        gradient_accumulation_steps=1\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "        \n",
    "    model,optimizer, train_dataloader, eval_dataloader, lr_scheduler, lpips_loss_fn = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler, lpips_loss_fn\n",
    "    )\n",
    "    step_count = 0\n",
    "    # image_save_steps = 1\n",
    "    image_save_steps = 100\n",
    "    acc_steps = 4\n",
    "    img_save_count = 0\n",
    "    vae_autoencoder_ckpt_name = 'vqvae_autoencoder_ckpt.pth'\n",
    "\n",
    "    kl_scale = 1e-6\n",
    "    lpips_scale = 1e-1\n",
    "    \n",
    "    model = accelerator.unwrap_model(model)\n",
    "    # Now you train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        losses = []\n",
    "        mse_losses = []\n",
    "        kl_losses = []\n",
    "        lpips_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        for batch in train_dataloader:\n",
    "            step_count += 1\n",
    "            x = batch[\"input\"]\n",
    "            with accelerator.accumulate(model):\n",
    "                posterior = model.encode(x).latent_dist\n",
    "                z = posterior.mode()\n",
    "                pred = model.decode(z).sample\n",
    "                kl_loss = posterior.kl().mean()\n",
    "                mse_loss = F.mse_loss(pred, x, reduction=\"mean\")\n",
    "                lpips_loss = lpips_loss_fn(pred, x).mean()\n",
    "\n",
    "                loss = (\n",
    "                    mse_loss + lpips_scale * lpips_loss + kl_scale * kl_loss\n",
    "                )\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                # if step_count % acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                mse_losses.append(mse_loss.item())\n",
    "                kl_losses.append(kl_loss.item())\n",
    "                lpips_losses.append(lpips_loss.item())\n",
    "\n",
    "                progress_bar.set_postfix(\n",
    "                    loss=loss.item(), \n",
    "                    mse_loss=mse_loss.item(), \n",
    "                    kl_loss=kl_loss.item(),\n",
    "                    lpips_loss=lpips_loss.item(),\n",
    "                    step=step_count\n",
    "                )\n",
    "                progress_bar.update(1)\n",
    "                # Image Saving Logic\n",
    "                if step_count % image_save_steps == 0 or step_count == 1:\n",
    "                    images = []\n",
    "                    with torch.no_grad():\n",
    "                        vae_model = accelerator.unwrap_model(model)\n",
    "                        eval_batch = next(iter(eval_dataloader))\n",
    "                        eval_x = eval_batch[\"input\"]\n",
    "                        reconstructions = vae_model(eval_x).sample\n",
    "                        sample_size = min(8, eval_x.shape[0])\n",
    "                        img_tensor = torch.cat([eval_x.cpu(), reconstructions.cpu()], axis=0)\n",
    "                        images = pt_to_pil(eval_x.cpu()) + pt_to_pil( reconstructions.cpu())\n",
    "                        # print(img_tensor.shape)\n",
    "                        # print(sample_size)\n",
    "                        # grid = make_grid(img_tensor, nrow=sample_size)\n",
    "                        # print(grid.shape)\n",
    "                        # img = transforms.ToPILImage()(grid)\n",
    "                        img = make_image_grid(images, rows=2, cols=4)\n",
    "                        if not os.path.exists(os.path.join(task_name,'vqvae_autoencoder_samples')):\n",
    "                            os.mkdir(os.path.join(task_name, 'vqvae_autoencoder_samples'))\n",
    "                        img.save(os.path.join(task_name,'vqvae_autoencoder_samples',\n",
    "                                            'current_autoencoder_sample_{}.png'.format(img_save_count)))\n",
    "                        img_save_count += 1\n",
    "                        img.close()\n",
    "                \n",
    "                    \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        avg_epoch_loss = sum(losses) / len(losses)\n",
    "        avg_epoch_mse_loss = sum(mse_losses) / len(mse_losses)\n",
    "        avg_epoch_kl_loss = sum(kl_losses) / len(kl_losses)\n",
    "        avg_epoch_lpips_loss = sum(lpips_losses) / len(lpips_losses)\n",
    "        progress_bar.set_postfix(\n",
    "            avg_epoch_loss=avg_epoch_loss, \n",
    "            avg_epoch_mse_loss=avg_epoch_mse_loss, \n",
    "            avg_epoch_kl_loss=avg_epoch_kl_loss,\n",
    "            avg_epoch_lpips_loss=avg_epoch_lpips_loss,\n",
    "            step=step_count\n",
    "        )\n",
    "        model.save_pretrained(os.path.join(task_name, \"model\"))\n",
    "        # torch.save(model.state_dict(), os.path.join(task_name, vae_autoencoder_ckpt_name))\n",
    "    print('Done Training...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5010fd8d-6478-431a-94fb-da5bb669bbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/diffusers/utils/outputs.py:64: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pthLoading model from: /nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pthLoading model from: /nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "\n",
      "\n",
      "Loading model from: /nfshomes/skarki/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f3ebb76b7a472c805668b42390ce07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacfe14ab26e46d398b20eaed4f3948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90a372c983a4768be4f31fb08f528f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fe2010f0c5459b80b2005357cf1181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m (train_dataloader, eval_dataloader)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/accelerate/launchers.py:186\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/llm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m error_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentinel \u001b[38;5;129;01min\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/llm/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/llm/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = (train_dataloader, eval_dataloader)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db306b-16e2-4b8a-b585-71e6813fe7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
